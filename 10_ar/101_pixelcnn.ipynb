{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e40d74fc899aa3bec3a0e5ab0fc36f7b",
     "grade": false,
     "grade_id": "cell-3c98ddabe9e64f07",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Number of points for this notebook:</b> 3\n",
    "<br>\n",
    "<b>Deadline:</b> May 18, 2020 (Monday). 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 10.1. Autoregressive generative models. PixelCNN\n",
    "\n",
    "The goal of this exercise is to get familiar with autoregressive generative models using the PixelCNN model as an example.\n",
    "\n",
    "The model is decribed in Section 3.5 of [this paper](https://arxiv.org/pdf/1601.06759.pdf).\n",
    "\n",
    "**This exercise requires a significant amount of computing power, you need to use a GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc0197afbbd904c2b006f24732e6e8a3",
     "grade": true,
     "grade_id": "cell-a54f4cac48b8daec",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import tools\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is ../data\n"
     ]
    }
   ],
   "source": [
    "# When running on your own computer, you can specify the data directory by:\n",
    "# data_dir = tools.select_data_dir('/your/local/data/directory')\n",
    "data_dir = tools.select_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device for training (use GPU if you have one)\n",
    "device = torch.device('cuda:0')\n",
    "#device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b79b793e3771be4f29f1e582f8d5dfc6",
     "grade": false,
     "grade_id": "cell-6eeffe49baead231",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "767ca562df4a44a61a6bd37995d6c9c2",
     "grade": false,
     "grade_id": "cell-94c5742c02305758",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Data\n",
    "\n",
    "In this exercise, we use standard MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c68960f7f54eb8281e78b61a10c0e5d3",
     "grade": false,
     "grade_id": "cell-532a4922e89ce5f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e4a44a191d17e9ab58c63a010205cff",
     "grade": false,
     "grade_id": "cell-8435e1b2ac1ef0fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = iter(trainloader).next()\n",
    "tools.plot_images(images[:8], ncol=4, cmap=plt.cm.Greys, clim=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b921f2a3c2e972336d40fba073a5a6c7",
     "grade": false,
     "grade_id": "cell-a4de8a0f0588b4df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# PixelCNN\n",
    "\n",
    "<img src=\"pixelcnn_context.png\" width=200 style=\"float: right; padding:20px;\">\n",
    "\n",
    "PixelCNN is an autoregressive model for the distribution of images.\n",
    "The joint probability $p(x)$ of an $n \\times n$ image $x$ is written as a product\n",
    "of the conditional distributions over the pixels:\n",
    "$$\n",
    "  p(x) = \\prod_{i=1}^{n^2} p(x_i|x_1,...,x_{i-1})\n",
    "$$\n",
    "The order of the pixels in the model is chosen arbitrarily. It is convenient to choose the first pixel $x_1$ to be in the top left corner and the last pixel $x_{n^2}$ in the bottom right corner (see the figure).\n",
    "\n",
    "We model the conditional distribution $p(x_i|x_1,...,x_{i-1})$ using a deep convolutional neural network. This network is designed in the following way:\n",
    "- The input and the output images have the same size.\n",
    "- The value of pixel $i$ in the output image is only affected by pixels of the input image that precede $i$ (as shown on the figure). This can be achieved by a network which is a stack of masked convolutional layers.\n",
    "\n",
    "# Masked convolutional layer\n",
    "\n",
    "- A masked convolutional layer is a standard convolutional layer whose kernel has zero values below and to the right of the central location. The remaining values of the kernel are the parameters of the layer which are trained in a standard way.\n",
    "\n",
    "- A simple way to implement the masked convolutional layer is to use a standard `nn.Conv2d` module and multiply its kernel by a binary mask in the `forward()` function.\n",
    "\n",
    "- The layer can have two kinds of binary masks:\n",
    "  1. with zero in the center (`blind_center=True`):\n",
    "<img src=\"masked_conv.png\" width=150>\n",
    "  2. with one in the center (`blind_center=False`):\n",
    "<img src=\"masked_conv_2.png\" width=150>\n",
    "\n",
    "  We will use the first type of mask in the first layer of our PixelCNN model and the second type of mask in the remaining layers. This kind of masking ensures that the output pixels are not affected by subsequent pixels of the input image.\n",
    "\n",
    "- You can create the binary mask using function [`register_buffer`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.register_buffer). This way the mask will be automatically transferred to the given device when calling `model.to(device)`.\n",
    "\n",
    "- The convolutional layer should **not** have a bias term because the biases are not affected by the mask.\n",
    "\n",
    "You need to implement the masked convolutional layer in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "752c45eec92ba10691fec890f3e760e5",
     "grade": false,
     "grade_id": "MaskedConv2d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, blind_center=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          in_channels (int): Number of input channels.\n",
    "          out_channels (int): Number of output channels.\n",
    "          kernel_size (int): Kernel size similar to nn.Conv2d layer.\n",
    "          blind_center (bool): If True, the kernel has zero in the center.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        super(MaskedConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=int((kernel_size-1)/2),bias=False)\n",
    "        self.register_buffer('mask', self.conv.weight.data.clone())\n",
    "        _, _, kH, kW = self.conv.weight.size()\n",
    "        self.mask.fill_(1)\n",
    "        kH, kW = kH //2, kW //2\n",
    "        if blind_center:\n",
    "            self.mask[:, :, kH + 1:] = 0\n",
    "            self.mask[:, :, kH, kW:] = 0\n",
    "        else:\n",
    "            self.mask[:, :, kH + 1:] = 0\n",
    "            self.mask[:, :, kH , kW + 1:] = 0\n",
    "    \n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (batch_size, in_channels, height, width): Input images.\n",
    "        \n",
    "        Returns:\n",
    "          y of shape (batch_size, out_channels, height, width): Output images.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        self.conv.weight.data = self.conv.weight.data * self.mask\n",
    "        return self.conv(x)\n",
    "    \n",
    "#        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "570f9929f347d3f834a47eac06bc8ef6",
     "grade": false,
     "grade_id": "cell-2eb3ff8f45ffe184",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_MaskedConv2d_shapes():\n",
    "    layer = MaskedConv2d(in_channels=1, out_channels=2, kernel_size=5, blind_center=False)\n",
    "    x = torch.ones(1, 1, 28, 28)\n",
    "    y = layer(x)\n",
    "    assert y.shape == torch.Size([1, 2, 28, 28]), f\"Bad y.shape: {y.shape}\"\n",
    "\n",
    "test_MaskedConv2d_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25fccc1ff7ff7416fa4c77384ed1b2b6",
     "grade": true,
     "grade_id": "test_MaskedConv2d",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:\n",
      " tensor([[-0.0800, -0.1600, -0.2400, -0.2400, -0.2400, -0.2400, -0.2400, -0.2400],\n",
      "        [-0.3200, -0.4800, -0.6400, -0.6400, -0.6400, -0.6400, -0.5600, -0.4800],\n",
      "        [-0.5600, -0.8000, -1.0400, -1.0400, -1.0400, -1.0400, -0.8800, -0.7200],\n",
      "        [-0.5600, -0.8000, -1.0400, -1.0400, -1.0400, -1.0400, -0.8800, -0.7200],\n",
      "        [-0.4000, -0.4800, -0.5600, -0.5600, -0.5600, -0.5600, -0.4000, -0.2400],\n",
      "        [ 0.0800,  0.1600,  0.2400,  0.2400,  0.2400,  0.2400,  0.2400,  0.2400],\n",
      "        [ 0.5600,  0.8000,  1.0400,  1.0400,  1.0400,  1.0400,  0.8800,  0.7200],\n",
      "        [ 0.5600,  0.8000,  1.0400,  1.0400,  1.0400,  1.0400,  0.8800,  0.7200]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "expected:\n",
      " tensor([[-0.0800, -0.1600, -0.2400, -0.2400, -0.2400, -0.2400, -0.2400, -0.2400],\n",
      "        [-0.3200, -0.4800, -0.6400, -0.6400, -0.6400, -0.6400, -0.5600, -0.4800],\n",
      "        [-0.5600, -0.8000, -1.0400, -1.0400, -1.0400, -1.0400, -0.8800, -0.7200],\n",
      "        [-0.5600, -0.8000, -1.0400, -1.0400, -1.0400, -1.0400, -0.8800, -0.7200],\n",
      "        [-0.4000, -0.4800, -0.5600, -0.5600, -0.5600, -0.5600, -0.4000, -0.2400],\n",
      "        [ 0.0800,  0.1600,  0.2400,  0.2400,  0.2400,  0.2400,  0.2400,  0.2400],\n",
      "        [ 0.5600,  0.8000,  1.0400,  1.0400,  1.0400,  1.0400,  0.8800,  0.7200],\n",
      "        [ 0.5600,  0.8000,  1.0400,  1.0400,  1.0400,  1.0400,  0.8800,  0.7200]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "tests.test_MaskedConv2d(MaskedConv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "914ac647df1862e066f62a23af79795b",
     "grade": false,
     "grade_id": "cell-2583df316cfae7fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e0d6eec108>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect the receptive field of one neuron in the masked convolutional layer\n",
    "layer = MaskedConv2d(in_channels=1, out_channels=2, kernel_size=5, blind_center=False)\n",
    "\n",
    "# Receptive field for output pixel at location (5, 5), input-output images of size (10x10)\n",
    "i, j = 5, 5\n",
    "rfield = tests.get_binary_receptive_field(layer, image_size=(10, 10), i=i, j=j)\n",
    "plt.matshow(rfield, cmap=plt.cm.Purples, clim=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7fdfd2fc68f88fa5f0b4f43411d80664",
     "grade": false,
     "grade_id": "cell-40178f9437508411",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Visualize receptive field of many layers stacked on top of each other\n",
    "\n",
    "Let us stack a few `MaskedConv2d` layers on top of each other and visualize the receptive field for one of the output pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9622d48280c4eecc9929bba8d82e1998",
     "grade": false,
     "grade_id": "cell-fbee53fdbb291d10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Stack multiple layers\n",
    "net = nn.Sequential(\n",
    "    MaskedConv2d(in_channels=1, out_channels=2, kernel_size=5, blind_center=True),\n",
    "    MaskedConv2d(in_channels=2, out_channels=2, kernel_size=5, blind_center=False),\n",
    "    MaskedConv2d(in_channels=2, out_channels=2, kernel_size=5, blind_center=False),\n",
    "    MaskedConv2d(in_channels=2, out_channels=2, kernel_size=5, blind_center=False),\n",
    "    MaskedConv2d(in_channels=2, out_channels=2, kernel_size=5, blind_center=False),\n",
    "    MaskedConv2d(in_channels=2, out_channels=2, kernel_size=5, blind_center=False),\n",
    "    MaskedConv2d(in_channels=2, out_channels=2, kernel_size=5, blind_center=False),\n",
    "    MaskedConv2d(in_channels=2, out_channels=2, kernel_size=5, blind_center=False),\n",
    "    nn.Conv2d(2, 256, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11534c278eec6dfe4e9c8691af046d74",
     "grade": false,
     "grade_id": "cell-45fef25923d78708",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e0dc9cf0c8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize receptive field\n",
    "rfield = tests.get_binary_receptive_field(net, image_size=(28, 28), i=13, j=13)\n",
    "plt.matshow(rfield, cmap=plt.cm.Purples, clim=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ea025b469ca48254629548e6bdac83a",
     "grade": false,
     "grade_id": "cell-2bbec1a7968dfa54",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If your implementation of the masked convolutional layer is correct, you should see the blind spot problem: Some of the preceding pixels do not affect the value of the pixel at (13, 13). We will not try to address the blind-spot problem in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d8029bf42e87322c76ebe22d709d6767",
     "grade": false,
     "grade_id": "cell-2f7bb5fcd7aaf3ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# PixelCNN model\n",
    "\n",
    "Next you need to implement the PixelCNN model which is simply a stack of `MaskedConv2d` layers.\n",
    "\n",
    "The recommended architecture is:\n",
    "* Masked convolutional layer with 1 input channel, `n_channels` output channels, given `kernel_size`, `blind_center=True`\n",
    "* 2d batch normalization followed by ReLU nonlinearity\n",
    "* 7 blocks with:\n",
    "    * Masked convolutional layer with `n_channels` input channels, `n_channels` output channels, given `kernel_size`, `blind_center=False`\n",
    "    * 2d batch normalization followed by ReLU nonlinearity\n",
    "* $1 \\times 1$ convolution with `n_channels` input channels and 256 output channels.\n",
    "* **Do not use the softmax nonlinearity in the forward function. Our tests assume that the loss function is [`nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss) with the log-softmax implemented there.**\n",
    "\n",
    "Hints:\n",
    "* We do not test the architecture of the PixelCNN model.\n",
    "* **We recommend you to check receptive fields of different outputs in your PixelCNN model to make sure that the outputs are not affected by subsequent pixels (pixels below and to the right).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1f95134e0f94bc6b0b2756cb5ef86fb",
     "grade": false,
     "grade_id": "cell-da2638d0d00a0d55",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### About using 2d batch normalization in the model\n",
    "\n",
    "Using the batch normalization significantly improves the convergence of the training procedure. However, when the network is in the `train()` mode (that means that the batch norm uses statistics computed from the mini-batch), the batch norm breaks the required causality structure of the PixelCNN model. Since all the locations contribute to the batch statistics, the subsequent pixels affect the values of the previous pixels. Even though, the batch normalization represents the whole batch using only two statistics, the network seems to learn to make use of the information in the subsequent pixels. This is a possible explanation of the following observation: the loss computed in the `eval()` mode can be substantially larger compared to the loss computed in the `train()` mode.\n",
    "\n",
    "When the network is used in the `eval()` model, the required causality structure is preserved. Even though using running statistics may result in larger loss values, it does not seem to affect significantly the quality of the generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f0a855a88ebdffdbce4dc65366b4cba",
     "grade": false,
     "grade_id": "PixelCNN",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, n_channels=64, kernel_size=7):\n",
    "        \"\"\"PixelCNN model.\"\"\"\n",
    "        super(PixelCNN, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        def _block(num):\n",
    "            return nn.Sequential(\n",
    "                MaskedConv2d(n_channels if num != 0 else 1, n_channels, kernel_size, blind_center=not bool(num)),\n",
    "                nn.BatchNorm2d(n_channels),\n",
    "                nn.ReLU()\n",
    "                )\n",
    "        self.model = nn.Sequential(\n",
    "            *[_block(i) for i in range(8)],\n",
    "            nn.Conv2d(n_channels, 256, kernel_size=1)\n",
    "        )\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Compute logits of the conditional probabilities p(x_i|x_1, ..., x_{i-1}) of the PixelCNN model.\n",
    "        \n",
    "        Args:\n",
    "          x of shape (batch_size, 1, 28, 28): Tensor of input images.\n",
    "        \n",
    "        Returns:\n",
    "          logits of shape (batch_size, 256, 28, 28): Tensor of logits of the conditional probabilities\n",
    "                                                      for each pixel.\n",
    "        \n",
    "        NB: Do not use softmax nonlinearity after the last layer.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return self.model(x)\n",
    "        #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a6055abaf2b3ffca0cd5a8f4c69e307",
     "grade": false,
     "grade_id": "cell-5658f49d7dd9d061",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_PixelCNN_shapes():\n",
    "    net = PixelCNN(n_channels=64, kernel_size=7)\n",
    "\n",
    "    batch_size = 32\n",
    "    x = torch.randn(batch_size, 1, 28, 28)\n",
    "    y = net(x)\n",
    "    assert y.shape == torch.Size([batch_size, 256, 28, 28]), f\"Bad y.shape: {y.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_PixelCNN_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b062aa6015d509b927c466c0bbe0dbb5",
     "grade": true,
     "grade_id": "cell-9f68a15cf4d6e00c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests that the outputs are not affected by subsequent pixels (pixels below and to the right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec777a3f2cf86346639b79dac9f0b039",
     "grade": false,
     "grade_id": "cell-dcd29d63fda524ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Loss function for training PixelCNN\n",
    "\n",
    "Next we implement the loss function used to train the PixelCNN model. Note that in PixelCNN, the conditional distributions $p(x_i|x_1,...,x_{i-1})$ of pixel intensities $x_i$ are multinomial distributions over 256 possible values. Thus, the loss function is the mean of the cross-entropy classification losses with 256 classes computed for each pixel $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc1ea8d130ea32aa45417a8a8a76613c",
     "grade": false,
     "grade_id": "loss_fn",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def loss_fn(logits, x):\n",
    "    \"\"\"Compute PixelCNN loss. The PixelCNN model uses conditional distributions $p(x_i|x_1,...,x_{i-1})$\n",
    "    for pixel intensities x_i which are multinomial distributions over 256 possible values. Thus the loss\n",
    "    function is the cross-entropy classification loss with 256 intensity values computed for each pixel x_i.\n",
    "\n",
    "    NB: Our tests assume the cross-entropy loss function which has log_softmax implemented inside,\n",
    "    such as `nn.CrossEntropyLoss`.\n",
    "\n",
    "    Args:\n",
    "      logits of shape (batch_size, 256, 28, 28): Logits of the conditional probabilities\n",
    "                  p(x_i | x_1,...,x_{i-1}) of the 256 intensities of pixel x_i computed using all\n",
    "                  previous pixel value x_1,...,x_{i-1}.\n",
    "      x of shape (batch_size, 1, 28, 28): Images used to produce `generated_x`. The values of pixel\n",
    "                  intensities in x are between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "      loss: Scalar tensor which contains the value of the loss.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    x = x.to(device)\n",
    "    y = (x * 255).long().squeeze(1)\n",
    "    return F.cross_entropy(logits.to(device), y)\n",
    "    \n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e3de67fb8e5810257ad48e526138c25",
     "grade": true,
     "grade_id": "test_loss_fn",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(5.5452)\n",
      "expected: tensor(5.5452)\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Test of PixelCNN loss\n",
    "def test_loss_fn():\n",
    "    net = PixelCNN(n_channels=1, kernel_size=5)\n",
    "\n",
    "    generated_x = torch.ones(1, 256, 28, 28)\n",
    "    x = .5 * torch.ones(1, 1, 28, 28)\n",
    "    loss = loss_fn(generated_x, x)\n",
    "    expected = torch.tensor(5.5452)\n",
    "    \n",
    "    print('loss:', loss)\n",
    "    print('expected:', expected)\n",
    "    assert torch.allclose(loss, expected), \"loss does not match expected value.\"\n",
    "    print('Success')\n",
    "\n",
    "test_loss_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7fff80bfa38b1b14c223f6bb8005270a",
     "grade": false,
     "grade_id": "cell-bc168c6bfd9e0281",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Generation procedure\n",
    "\n",
    "Next we implement the procedure that generates samples using a PixelCNN model. The generation proceeds as follows:\n",
    "* Initialize `samples` tensor as images with all zeros.\n",
    "* Apply the PixelCNN model to `samples` tensor. The output will contain logits (probabilities before softmax) over 256 pixel intensity values for pixels in all locations. However, on the first iteration we are only interested in the pixel intensities at the first location (0,0) because we need to compute\n",
    "$$p(x_1)$$.\n",
    "* Use computed probabilities to sample a pixel intensity value for the pixel at location (0, 0). Write the sampled value to location (0, 0) of the `samples` tensor.\n",
    "* Apply the model to the `samples` tensor. Now the sampled value of $x_1$ is used by the model to generate the probabilities of pixel intensities for the pixel at location (0, 1), thus we computed\n",
    "$$p(x_2\\mid x_1).$$\n",
    "\n",
    "* We sample a pixel intensity value for the second pixel and write it to the corresponding location of `samples` tensor.\n",
    "* We continue until we change all the values of the `samples` tensor.\n",
    "\n",
    "Hints:\n",
    "* **Do not forget to set the model into the evaluation mode by `net.eval()`.**\n",
    "* Remember that the pixels of the generated images should have values between 0 and 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e76dc9f34ba4845667a28ba157ca283",
     "grade": false,
     "grade_id": "cell-b5294d858504f686",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generate(net, n_samples, image_size=(28, 28), device='cpu'):\n",
    "    \"\"\"Generate samples using a trained PixelCNN model.\n",
    "\n",
    "    Args:\n",
    "      net:        PixelCNN model.\n",
    "      n_samples:  Number of samples to generate.\n",
    "      image_size: Tuple of image size (height, width).\n",
    "      device:     Device to use.\n",
    "    \n",
    "    Returns:\n",
    "      samples of shape (n_samples, 1, height, width): Generated samples.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    net.eval()\n",
    "    samples = torch.Tensor(n_samples, 1, image_size[0], image_size[1]).to(device)\n",
    "    samples.fill_(0)\n",
    "    h,w= image_size\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            output = net(samples)[:, :, i, j]\n",
    "            samples[:, :, i, j] = torch.multinomial(F.softmax(output), 1).float() / 255.0\n",
    "    \n",
    "    return samples\n",
    "    \n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f97eaa45dcff576009d8000bd2a4e3f",
     "grade": true,
     "grade_id": "test_generate",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples[0, 0, 0]:\n",
      " tensor([0.0039, 0.0078, 0.0118, 0.0157, 0.0196, 0.0235, 0.0275, 0.0314, 0.0353,\n",
      "        0.0392, 0.0431, 0.0471, 0.0510, 0.0549, 0.0588, 0.0627, 0.0667, 0.0706,\n",
      "        0.0745, 0.0784, 0.0824, 0.0863, 0.0902, 0.0941, 0.0980, 0.1020, 0.1059,\n",
      "        0.1098])\n",
      "expected:\n",
      " tensor([0.0039, 0.0078, 0.0118, 0.0157, 0.0196, 0.0235, 0.0275, 0.0314, 0.0353,\n",
      "        0.0392, 0.0431, 0.0471, 0.0510, 0.0549, 0.0588, 0.0627, 0.0667, 0.0706,\n",
      "        0.0745, 0.0784, 0.0824, 0.0863, 0.0902, 0.0941, 0.0980, 0.1020, 0.1059,\n",
      "        0.1098])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Test of generation\n",
    "def test_generate():\n",
    "    net = PixelCNN(n_channels=1, kernel_size=5)\n",
    "    \n",
    "    # monkey-patching net.forward\n",
    "    def my_forward(x):\n",
    "        logits = torch.zeros(1, 256, 28, 28)\n",
    "        ix = min((x[0, :, :, :]*255 + 1).long().max(), 255)\n",
    "        logits[0, ix] = 100\n",
    "        return logits\n",
    "        \n",
    "    net.forward = my_forward\n",
    "    \n",
    "    samples = generate(net, n_samples=1, image_size=(28, 28), device='cpu')\n",
    "    expected = torch.arange(1, 29) / 255.\n",
    "    print('samples[0, 0, 0]:\\n', samples[0, 0, 0])\n",
    "    print('expected:\\n', expected)\n",
    "    assert torch.allclose(samples[0, 0, 0], expected), \"samples does not match expected value.\"\n",
    "    print('Success')\n",
    "\n",
    "test_generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "265e18d51e97f6a1d2c0ba9d382903b9",
     "grade": false,
     "grade_id": "cell-021921fe6c0305e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Train PixelCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16c10056399ef6ec1e4031b96b595a7d",
     "grade": false,
     "grade_id": "cell-a03c73f0dcaa5d60",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PixelCNN(\n",
       "  (model): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): MaskedConv2d(\n",
       "        (conv): Conv2d(1, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      )\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MaskedConv2d(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      )\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MaskedConv2d(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      )\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MaskedConv2d(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      )\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MaskedConv2d(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      )\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MaskedConv2d(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      )\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MaskedConv2d(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      )\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): MaskedConv2d(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      )\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (8): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a model\n",
    "net = PixelCNN(n_channels=64, kernel_size=7)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bae2a72d27d4429d8ac435fec329f39b",
     "grade": false,
     "grade_id": "cell-29dcaff49b84af33",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot generated samples for an untrained model\n",
    "# Note: generation on CPU may take a significant amount of time\n",
    "if not skip_training:\n",
    "    net.to(device)\n",
    "    samples = generate(net, n_samples=120, device=device)\n",
    "    tools.plot_generated_samples(samples, ncol=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c43e872f7856f6306147e0bc7332c134",
     "grade": false,
     "grade_id": "cell-517ca6cb2efc3c8b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training loop\n",
    "\n",
    "Implement the training loop in the cell below. The recommended hyperparameters:\n",
    "* Adam optimizer with learning rate 0.001\n",
    "* Number of epochs: 11. If you train for more epochs, the cost function may decrease further but the quality of the generated samples may degrade because of overfitting.\n",
    "\n",
    "Hints:\n",
    "- The loss at convergence can reach 0.63.\n",
    "- Please use this code to plot 120 generated samples after each epoch. This will allow you to track the training progress.\n",
    "```\n",
    "with torch.no_grad():\n",
    "    samples = generate(net, n_samples=120, device=device)\n",
    "    tools.plot_generated_samples(samples)\n",
    "```\n",
    "- The generated images may not be of great quality but you should definitely generate digit-like images.\n",
    "- **Do not forget to set the model into the training mode by `net.train()` before training.**\n",
    "- The generated samples are expected to look similar to this:\n",
    "<img src=\"pixelcnn_generated_samples.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae914389933e3986497ab27451391731",
     "grade": false,
     "grade_id": "training_loop",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    optim = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "    for epoch in range(11):\n",
    "        cum_loss = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            img, label = data\n",
    "            img = img.to(device)\n",
    "            net.train()\n",
    "    \n",
    "            optim.zero_grad()\n",
    "    \n",
    "            out = net(img)\n",
    "            loss = loss_fn(out, img)\n",
    "            loss.backward()\n",
    "    \n",
    "            optim.step()\n",
    "    \n",
    "            cum_loss += loss.item()\n",
    "    \n",
    "            if (i % 1000 == 999):\n",
    "                print('Epoch: {} Step: {} loss: {}'.format(epoch,i,cum_loss / 1000))\n",
    "                cum_loss = 0\n",
    "                with torch.no_grad():\n",
    "                    samples = generate(net, n_samples=120, device=device)\n",
    "                    tools.plot_generated_samples(samples)\n",
    "    \n",
    "   \n",
    "    \n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "648ef7545f0e5fb802fc40d8c2ad5e42",
     "grade": false,
     "grade_id": "cell-d76b73a069912948",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from 10_pixelcnn.pth.\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "if not skip_training:\n",
    "    tools.save_model(net, '10_pixelcnn.pth')\n",
    "else:\n",
    "    net = PixelCNN(n_channels=64, kernel_size=7)\n",
    "    tools.load_model(net, '10_pixelcnn.pth', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24ba8387e2ab6cd199e9b0f8f6e10ed8",
     "grade": false,
     "grade_id": "cell-803a1643f4f2ed4b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "237ed8504dff76ab8994fdb753b48304",
     "grade": false,
     "grade_id": "cell-34cb7b192e397c65",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    with torch.no_grad():\n",
    "        samples = generate(net, n_samples=120, device=device)\n",
    "        tools.plot_generated_samples(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e27dee031c91e9325c01f039518902c4",
     "grade": true,
     "grade_id": "cell-26b0db40715dad8a",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests the training loss of the trained PixelCNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d85bbefdb635d20419d07d309992854b",
     "grade": false,
     "grade_id": "cell-258078ae687b2e63",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Conclusion</b>\n",
    "</div>\n",
    "\n",
    "In this notebook, we learned how to train PixelCNN, an autoregressive generative model of images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
